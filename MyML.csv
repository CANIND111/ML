X,Y
1.KNNREGRESSOR Scratch,"LIBRARIES from sklearn.datasets import fetch_california_housing ; import pandas as pd ; import numpy as np / DATA housing = fetch_california_housing() ; df = pd.DataFrame(housing.data, columns = housing.feature_names) ; df.shape / X = df[['AveRooms','HouseAge']] ; y = housing.target ; X.head() / SPLIT from sklearn.model_selection import train_test_split ; X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2 , random_state = 110) / FUNCTIONS== def predict_price(X_train,y_train,sample,p, k=5): / distances = [] /  for x in X_train.to_numpy():  / IND distance =  minkowski_distance(sample,x,p) / IND  distances.append(distance)  / RI  sorted_distances = np.argsort(distances) /  k_nearest_targets = y_train[sorted_distances[:k]] /     predict_price = np.mean(k_nearest_targets) /  return predict_price / PREDICT  sample1 = X_test.iloc[1,:] ; k=11 ; predict_price(X_train,y_train,k,sample1) / y_test[1] /  Y_PREDS - y_preds= [] / for x in X_test.to_numpy(): /     y_pred = predict_price(X_train,y_train,k,x,3) /     y_preds.append(y_pred) / y_preds / MSE from sklearn.metrics import mean_squared_error / mse = mean_squared_error(y_test,y_preds) / mse # k=11/////  def calculate_mse_for_k(X_train,y_train,X_test,y_test,k,p): /   y_preds= [] /    for sample in X_test.to_numpy(): / IND  y_pred = ; predict_price(X_train,y_train,sample,k,p) / IND   y_preds.append(y_pred) / RI    mse = mean_squared_error(y_test,y_preds) ;/   return mse/  DIFFERENT MSE k_values = range(3,30) / mse_values= [] / for k in k_values:  /     mse = calculate_mse_for_k(X_train,y_train,X_test,y_test,k,p=3) /    mse_values.append(mse) / mse_values  "
2.KNNREGRESOR Sklearn,"from sklearn.neighbors import KNeighborsRegressor / knn_regressor = KNeighborsRegressor(n_neighbors=11,p=3) / knn_regressor.fit(X_train,y_train) / y_pred = knn_regressor.predict(X_test) / mse = mean_squared_error(y_test,y_pred) / mse  //// CALIFORNIA HOUSING from sklearn.datasets import fetch_california_housing / import pandas as pd / import numpy as np / from sklearn.neighbors import KNeighborsRegressor / from sklearn.metrics import mean_squared_error / housing = fetch_california_housing() / df = pd.DataFrame(housing.data, columns = housing.feature_names) / df.shape / X = df[['AveRooms','HouseAge']] / y = housing.target /X.head() / knn_regressor = KNeighborsRegressor(n_neighbors=11,p=3) / knn_regressor.fit(X_train,y_train) / y_pred = knn_regressor.predict(X_test) / mse = mean_squared_error(y_test,y_pred) / mse"
3.KNNCLASSIFIIER Scratch,"def predict_class(X_train,y_train,sample,p, k=5):  /    distances = [] /   for x in X_train.to_numpy():  /     IND   distance = minkowski_distance(sample,x,p)  /   IND distances.append(distance) / RI sorted_distances = np.argsort(distances) /    k_nearest_targets = y_train[sorted_distances[:k]] /     class_counts = np.bincount(k_nearest_targets) /   prediction = agrmax(class_counts) /     return prediction"
4.KKNCLASSIFIER Sklearn,"IRIS DATA import pandas as pd ; import numpy as np /  Loading data == from sklearn.datasets import load_iris ; from sklearn import neighbors ; from sklearn.model_selection import train_test_split ; from sklearn.neighbors import KNeighborsClassifier ; #features X = load_iris().data print(X) ; #target y = load_iris().target  y / # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25) ; # Create a KNN classifier with k=3 , knn = KNeighborsClassifier(n_neighbors=3) ; # Train the classifier , knn.fit(X_train, y_train) ; # Predict the labels of the test data , y_pred = knn.predict(X_test) ; print(""Prediction"",y_pred) / # Calculate the accuracy of the classifier , accuracy = knn.score(X_test, y_test) / # Print the accuracy
print(""Accuracy:"", accuracy)  LAB_COLORS import pandas as pd / import numpy as np / k=5 / # Data data = { /n  'Color': ['Blue', 'Blue', 'Blue', 'Blue', 'Blue', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange'], /n   'x2': [15, 12, 10, 6, 8, 20, 28, 25, 30, 35], /n    'y2': [13, 10, 17, 15, 25, 10, 13, 5, 10, 14] /n }  / df = pd.DataFrame(data) / black_point = {'x': 19, 'y': 14} / df['euclidean_distance'] = np.sqrt((df['x2'] - black_point['x'])**2 + (df['y2'] - black_point['y'])**2)   /  df_sorted = df.sort_values(by='euclidean_distance')  / df_k = df_sorted.head(k) / df_k['Color'].value_counts(ascending=False).index[0] IRIS DATA WITH FEATURES import pandas as pd / import numpy as np / from sklearn.model_selection import train_test_split / from sklearn.datasets import load_iris / # features X = load_iris().data / # target
y = load_iris().target / X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=12)  (test_size = , default 75-25 ) / from sklearn.neighbors import KNeighborsClassifier/ model1 = KNeighborsClassifier(n_neighbors=5) / model1.fit(X_train, y_train) / print(f""training accuracy:{model1.score(X_train, y_train):.2%}"") / print(f""testing accuracy: {model1.score(X_test, y_test):.2%}"") ////  IRIS DATA WITH FEATURES  from sklearn.datasets import load_iris / from sklearn.metrics import accuracy_score /from sklearn.neighbors import KNeighborsClassifier /import numpy as np / iris_data = load_iris() / load_iris().feature_names /df = pd.DataFrame(data=load_iris().data, columns=load_iris().feature_names) /df.shape  // X = df[['sepal length (cm)', 'sepal width (cm)']] / y = iris_data.target /X.head()  // from sklearn.model_selection import train_test_split / X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 110) #Randomly selected Data / knn_class = KNeighborsClassifier(n_neighbors = 3,p=3) / knn_class.fit(X_train,y_train) / y_pred = knn_class.predict(X_test) / y_pred / acc=accuracy_score(y_test,y_pred) / acc"
5.DECISIONTREE Scratch ," 1. LIST 5 INTERS AND SPLIT import numpy as np / from numpy.random import default_rng /  rnd_list = default_rng(110) / int_list = rnd_list.integers(low=5, high=50, size = 5) / int_list //// def splits(x):
    splits_points = []  /    x_sorted = sorted(x)  /    RI  for i in range(len(x)-1):  /    IND  mid_point = (x_sorted[i] + x_sorted[i+1])/2 /   IND    splits_points.append(mid_point) /  RI    return splits_points  //// 2. ARRAY SHAPE 5,3 ITERATE COLUMNS AND FIND SPLIT POINTS.  int_twoDlist = rnd_list.integers(low=5, high=100, size = (5,3))  / int_twoDlist // def splits_per_col(data): /  col_split_pts = {}  /   ncols = data.shape[1] /  RI   for c in range(ncols): /       IND  split_pts = splits(data[:,c]) /  IND      col_split_pts['col_'+ str(c)] = split_pts /    RI  return col_split_pts // splits_per_col(int_twoDlist)  3. ARRAY 5,2 SPLIT POINTS AND MAE FOR THE ROOT int_twoDlist = rnd_list.integers(low=5, high=100, size = (5,2)) /  int_twoDlist  // FUNCTION TODO REGULAR INDENTENT  def split_data(x,y,split_pt): /    mask = x > split_pt /     anti_mask = x < split_pt /    x_true = x[mask] /    y_true = y[mask] /    x_false = x[anti_mask] /    y_false = y[anti_mask] /     return x_true, y_true, x_false , y_false  // x_true, y_true, x_false , y_false =split_data(int_twoDlist[:,0],int_twoDlist[:,1], 70)  //  FUNCTION -->  def MAE(y, y_true, y_false): /     y_true_hat = np.mean(y_true) /     y_false_hat = np.mean(y_false) /     y_hat = np.mean(y) //       old_mae = np.mean(np.absolute(y-y_hat)) /    new_mae = len(y_true)/ len(y) * np.mean(np.absolute(y_true-y_true_hat)) + \       len(y_false)/ len(y) * np.mean(np.absolute(y_false-y_false_hat))  //    return old_mae, new_mae  /// MAE(int_twoDlist[:,1],y_true,y_false) ///  GET THE BEST ESPLIT = def get_best_split(data): /     x = data[:,0] /     y = data[:,1] /     split_pts = splits(x)  /    results = {} /     for point in split_pts: /      IND   x_true, y_true, x_false , y_false = split_data(x,y, point) /  IND   errors = MAE(y, y_true, y_false) /    IND  results[point] = errors/ RI   best_point = min(results,key=results.get) /   return results,best_point  //// get_best_split(int_twoDlist)  //// MORE THAN 1 FEATURE  def get_best_split_features(data): /   n_feature = data.shape[1] - 1  /  results = {}  /    for feature_idx in range(n_feature): /      x = data[:,feature_idx] /     y = data[:,-1]  /  split_pts = splits(x) --- Loop insde the loop indent-->      for point in split_pts: /      x_true, y_true, x_false , y_false = split_data(x,y, point)  /    errors = MAE(y, y_true, y_false) /   results[feature_idx,point] = errors / --indente al primer loop --->  best_point = min(results,key=results.get)  /   return results,best_point"
6.DECISIONTREESKLEARN,"from sklearn.tree import DecisionTreeRegressor  # code to create a decision tree for regression / from sklearn import tree      # code to visualize a decision tree / import matplotlib.pyplot as plt     # plotting library needed to draw the nodes and branches/  import pandas as pd    / x = [1.5, 1., 2., 3., 2.5] / y = [1, 1.5, 2.5, 2.5, 3.] / apt = pd.DataFrame({'x':x , ""y"":y}) / apt / X = apt.drop('y', axis =1)   # drops the 'y' column from our dataframe and saves the rest of the columns (here just 'x') in variable X / y = apt['y']    # selects the 'y' column and saves it in the variable y / regr = DecisionTreeRegressor(random_state=1234)  # this creates a generic decision tree model (sort of like a recipe) / model = regr.fit(X, y)  "
7.RANDOMFOREST Sklearn,"import pandas as pd / rent = pd.read_csv('rent-ideal.csv') ; rent.head(8) /  TRAINING X = rent[['bedrooms','bathrooms','latitude','longitude']] ; y = rent['price'] /  X.values y.values /  from sklearn.ensemble import RandomForestRegressor /  rf = RandomForestRegressor(n_estimators=10)  / rf.fit(X, y) / import numpy as np ; unknown_x = np.array([2, 1, 40.7957, -73.97]) ;  unknown_x / rf.predict([unknown_x]) / from sklearn.metrics import mean_absolute_error / predictions = rf.predict(X) / e = mean_absolute_error(y, predictions) / ep = e*100 / y.mean() / print(f""${e:.0f} average error; {ep:.2f}% error"") "
8.DISTANCES,"def euclidean_distance(point1, point2):  return np.sqrt(sum((point1-point2)**2)) / def manhattan_distance(point1, point2):     return np.sum(np.abs(point1-point2)) / def minkowski_distance(point1, point2,p):  return np.power(sum(np.abs(point1-point2)**p),1/p)
    
     
    "
9. RANDOM LISTS,"RANDOM LIST USING UNIFORM/FLOATS ; import numpy as np ; rng = np.random.default_rng(23) ; n2 = rng.uniform(2,20,10) ; n2 //// rng = np.random.default_rng(132) ; y = rng.uniform(2,10,10) ;pred = rng.uniform(2,10,10) -- range 2-10 and give me 10 #   /// INTEGERS import numpy as np * from numpy.random import default_rng * rnd_list = default_rng(110) ** int_list = rnd_list.integers(low=5, high=50, size = 5) "
10. METRICS REGRESSION,"MSE def mean_squared_error(actual, predicted):  /  error = actual - predicted  /   squared_error = error * error  //     mean_squared_error = squared_error.sum()/len(actual) /   return mean_squared_error  /// MAE def MAE(y,pred):    return np.mean(np.abs(y-pred))  /// R2 = def r_square1(y, pred): *     residual_sum_of_squares = ((y - pred) ** 2).sum()  /     total_sum_of_squares = ((y - y.mean()) ** 2).sum() /    r_squared = 1 - (residual_sum_of_squares / total_sum_of_squares) /     return r_squared //// R2 PROFESSOR def r2(y_true, y_pred): /     numerator = len(y_true) *MSE(y_true, y_pred) /     y_avg= np.mean(y_true) /     diff = y_true - y_avg  /     square = diff**2 /    sum_of_squares = np.sum(square) /     denominator = sum_of_squares /    r2 = 1 - (numerator/denominator) /     return r2 /// SLEARN from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score, precision_score, recall_score, r2_score"
11. METRICS CLASSIFICATION," EXAMPLE ; ACCURACY TP + TN / ALL( TP+TN+FP+FN)y   ; y = np.array([0,0,0,0,1,1,1,1]) ; pred = np.array([1,0,1,1,1,1,1,0])  /// tp = np.sum((y == 1) & (pred== 1)) ;  tn = np.sum((y == 0) & (pred== 0)) ; fp = np.sum((y == 1) & (pred== 0)) ; fn = np.sum((y == 0) & (pred== 1)) / def accuracy_score(tn,tp,fn,fp):   acs = (tn + tp) / (tn + tp + fn + fp) /     return acs.mean() //// PRECISSION def precision(tp,fn):     precision = tp / (tp+fn) /     return precision  //// RECALL def recall_score(tp,fp): /     recall = tp / (tp+fp) /     return recall /// F1 SCORE precision = tp / (tp+fn)  // recall_score = tp / (tp+fp)  ; def f1_score(precision, recall): /     f1_score = (2 *precision * recall_score) / (precision + recall_score) /    return f1_score  SKLEARN from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
12. SPLIT LINE AND COLUMNS," SPLIT 1 LINE def splits(x):  splits_points = [] /    x_sorted = sorted(x)  /     for i in range(len(x)-1):  IND    mid_point = (x_sorted[i] + x_sorted[i+1])/2 / IND     splits_points.append(mid_point)  /    return splits_points   SPLIT 2 MORE COLUMNS def splits_per_col(data): /    col_split_pts = {} /    ncols = data.shape[1] /    for c in range(ncols): / IND      split_pts = splits(data[:,c]) / IND         col_split_pts['col_'+ str(c)] = split_pts / RI     return col_split_pts"
13. PLOT MSE KNNREGRESSOR,"import matplotlib.pyplot as plt / plt. plot(k_values,mse_values, marker = 'o')  / plt.title(""Mean Squared Error dor Diffrenet k values"")  /plt.xlabel(""k"")  /plt.ylabel(""MSE"")  /plt.show()"
14. RANDOM FOREST SCRATCH ,"def simple_rf(data):  /n     bs,oob = bs_sampling(data)  /n     print(f""oob:"",oob)  /n    total_features = data.shape[1]-1  /n    rand_n_of_feaures = int(np.sqrt(total_features))  /n     selected_features = rand.choice(total_features,rand_n_of_feaures,replace=False)   /n    selected_features_with_y = np.append(selected_features,-1)  /n      sample_data = bs[:,selected_features_with_y]  /n    dt = create_tree(sample_data)  /n     sample_oob = oob[:,selected_features]  /n     preds = dt.predict(sample_oob) /n     return preds   /n  def bs_sampling(data):  /n     bs = rand.choice(data,data.shape[0],replace=True) /n     unique_bs = np.unique(bs, axis=0)  /n     oob=[]  /n      for row in data:  /n       if sum(np.isin(row,unique_bs))==0:  /n        oob.append(row)   /n     return bs,np.array(oob)  /n  from sklearn.tree import DecisionTreeRegressor /n def create_tree(data): /n     X = data[:,0:-1] /n     y = data[:,-1]  /n     dt = DecisionTreeRegressor()  /n     dt.fit(X,y)  /n     return dt    /n ###Extra average proportion of oob after x runs /n oob_samples =[] /n for i in range(1,100):  /n     bs = rand.choice(data,data.shape[0],replace=True) /n    n_unique_samples = np.unique(bs, axis=0).shape[0] /n     oob = (data.shape[0] - n_unique_samples) / data.shape[0]  #proportion of ooob  /n    oob_samples.append(oob) /n  np.mean(oob_samples)  "
15. CHE3 FEATURE IMPORTANCE,"Create a baseline model and get oob score / This tells us that: / - $R^2 = 1$ means our model is perfect;  / - $R^2 \approx 0$ means our model does no better than just predicting the average; - $R^2 \lt\lt 0$ means our model does worse than predicting the average. /  We can calculate the feature importances using a permutation method, which consists of the following steps: - use all features and establish a baseline value for $R^2$;- select one feature and randomly permute its values leaving all other features unchanged;- calculate the new value for $R^2$ with this one feature permuted; - calculate the change in $R^2$ from the baseline; and, - repeat for the other features.  /  We can also calculate the importance of the features using a dropped column, which consists of the following steps: - use all features and establish a baseline value for $R^2$; - select one feature and remove it from the data; - calculate the new value for $R^2$ with this one feature removed; - calculate the change in $R^2$ from the baseline; and, - repeat for the other features."
17. CHE3 FEATURE CODIGO,"X = rent[['bedrooms','bathrooms','latitude','longitude']]  /  y = rent['price'] / import numpy as np / unseen_sample = np.array([2,2,40,-73])  / preds = rf.predict(X) / e = mean_absolute_error(y,preds) /ep = e / y.mean() * 100  / print(f""${e:.0f} average error; {ep:.2f}% error"")  //// from sklearn.model_selection import train_test_split / X_train, X_test, y_train , y_test = train_test_split(X,y, test_size = 0.25)/ f = RandomForestRegressor(n_estimators = 10) / rf.fit(X_train,y_train) / preds = rf.predict(X_test) / e = mean_absolute_error(y_test,preds) / ep = e / y.mean() * 100 / print(f""${e:.0f} average error; {ep:.2f}% error"") ////  #feature importance / from rfpimp import * / features_ranking = importances(rf_100, X_test, y_test) /features_ranking /plot_importances(features_ranking) /features_ranking = importances(rf_100, X_test, y_test , features = ['bedrooms','bathrooms',['longitude','latitude']]) / plot_importances(features_ranking)////"
18. CH3 FEATURE CODIGO CLASSIFICATION,"#Classification from sklearn.datasets import load_breast_cancer / import pandas as pd/ cancer = load_breast_cancer() / cancer.keys() / X = cancer.data / y = cancer.target / df = pd.DataFrame(X, columns = cancer.feature_names) / df.head().T / features = ['radius error','texture error','concave points error','symmetry error',     'worst texture','worst smoothness','worst symmetry'] / df= df[features] / df.head()
X_train, X_test, y_train , y_test = train_test_split(df,y, test_size = 0.20) / from sklearn.ensemble import RandomForestClassifier / from sklearn.metrics import accuracy_score , confusion_matrix / cl =RandomForestClassifier(n_estimators=200) / cl.fit(X_train,y_train)/preds = cl.predict(X_test) /e = accuracy_score(y_test,preds) /print(f""{e*100:.2f}% accuracy"") / confusion_matrix(y_test,preds) /features_ranking = importances(cl, X_test, y_test) / features_ranking   //// df_num = df.select_dtypes(include=['number'])  / df_num2 = df_num.drop(['GarageYrBlt' , 'LotFrontage', 'MasVnrArea'], axis = 1)   / df_num2 df_nonum = df.select_dtypes(include=['object']) / df_nonum"
19. CATEGORICAL VARS,"from rfpimp import * / def evaluate(X, y): /     rf = RandomForestRegressor(n_estimators=100, n_jobs=-1, oob_score=True) /     rf.fit(X, y) /     oob = rf.oob_score_ /     n = rfnnodes(rf) /     h=np.median(rfmaxdepths(rf)) /     print(f""OOB R^2 is {oob:.5f} using {n:,d} tree nodes with {h} median tree depth"") /     return rf, oob   //// def showimp(rf, X, y): /     features = list(X.columns) /    features.remove('latitude') /     features.remove('longitude') /     features += [['latitude','longitude']] /     I = importances(rf, X, y, features=features) /     plot_importances(I, color='#4575b4')  / evaluate(X, y) / showimp(rf, X, y) //// "
20. CAT VARS,"PERTMUTATION def perm_importances(X, y): /     rf = RandomForestRegressor(n_estimators=100, n_jobs=-1, oob_score=True, random_state=999) /     rf.fit(X, y) /     r2 = rf.oob_score_ /     print(f""Baseline R^2 with no columns permuted: {r2:.5f}\n"") /     for col in X.columns: / IND         X_col = X.copy() /         X_col[col] = X_col[col].sample(frac=1).values /         rf.fit(X_col, y) /         r2_col = rf.oob_score_ /         print(f""Permuting column {col}: new R^2 is {r2_col:.5f} and difference from baseline is {r2 - r2_col:.5f}"") / perm_importances(X, y)  ///// DROPPING  def drop_importances(X, y): /     rf =RandomForestRegressor(n_estimators=100, n_jobs=-1, oob_score=True, random_state=999) /     rf.fit(X, y) /     r2 = rf.oob_score_ /     print(f""Baseline R^2 with no columns dropped: {r2:.5f}\n"") /    for col in X.columns: / IND         X_col = X.copy() /         X_col = X_col.drop(col, axis=1)  /         rf.fit(X_col, y) /         r2_col = rf.oob_score_ /         print(f""Dropping column {col}: new R^2 is {r2_col:.5f} and difference from baseline is {r2 - r2_col:.5f}"") / drop_importances(X, y) //// "
21. EXTRA CORRELATION,"X_dup = X.copy() / X_dup['bedrooms_dup'] = X_dup['bedrooms'] / X_dup.head() / drop_importances(X_dup, y)  /////  BREAKING CORR noise = np.random.normal(0, 2, X_dup.shape[0]) / 
X_dup['bedrooms_dup'] = X_dup['bedrooms'] + noise / X_dup.head() / drop_importances(X_dup, y) ////"
22. oob runs ,"oob_score_3=[] / for i in range(10): /     rf_3= RandomForestRegressor(n_estimators=100, n_jobs= -1, oob_score= True) /     rf_3= rf3.fit(X_train, y_train) /    oob_score_3 .append(rf3.oob_score_) /// avg_oob_score_3 = np.mean(oob_score_3) / print(f""Average OOB score over 10 runs for model 3: {avg_oob_score_3:.4f}"")"
23. KEVAL ," data1.sample(n=91,random_state=54) / y = sample[[""sex""]] / x = sample.drop('sex', axis=1) / cl.feature_importances_ / cl.feature_names_in_ / pd.Series(index = cl.feature_names_in_, data = cl.feature_importances_) //// MAP df['Method'] = df['Method'].map({'S':45, 'VB':23, 'SP':67, 'PI':12, 'SA':55}) / sample1= df.sample(n =500 , random_state = 67) / sample1.iloc[367] //// df_onehot = pd.get_dummies(data=df, columns= ['Method'] , dtype = 'int') / df_onehot / sample = df_onehot.sample(n=250 , random_state =89).iloc[[58]]/  sample  ///pd.get_dummies(data=df, columns= ['Method'] , dtype = 'int').sample(n=250 , random_state =89).iloc[[58]] //// EUCLIDENA df['x1'] = -37.72739 / df['y1'] = 144.95303 / df[""euclidean_distance""] = np.sqrt((df['Lattitude'] - df['x1'])**2 + (df['Longtitude']- df['y1'])**2)/ df.iloc[500]"

x,y
KR 1 import train split,"from sklearn.datasets import fetch_california_housing /import pandas as pd / import numpy as np /housing= fetch_california_housing() /housing.data / x = pd.DataFrame(housing.data, columns = housing.feature_names )x / y = pd.DataFrame(housing.target , columns = ['price']) /y / from sklearn.model_selection import train_test_split / x_train, x_test, y_train, y_test = train_test_split(x,y,test_size= 0.2 , random_state = 110) / "
KR 2  func distances,"def distances(sample,x_train, y_train,p,k): /   distances = [] /   for i in range(len(x_train)): / ind     distance = minkowsky(sample,x_train.iloc[i],p) / ind    distances.append(distance) / ri   idx = np.argsort(distances)/   pred =np.mean(y_train.iloc[idx][:k]) /  return pred  / example usage distances(sample,x_train,y_train,2,4) / "
KR 3 func knn ,"from sklearn.metrics import mean_squared_error / def knn_regression(x_test, x_train, y_test,y_train,p,k): /  preds = []  /  for s in range(len(x_test)): / ind     pred = distances(x_test.iloc[s],x_train,y_train, p,k) / ind     preds.append(pred) / ri  return mean_squared_error(y_test,preds) / example usage knn_regression(x_test,x_train,y_test,y_train,2,6)"
KC 1 ,"from sklearn.metrics import accuracy_score / from sklearn.datasets import load_iris / iris = load_iris() / x = iris.data / y = iris.target  /x_train, x_test, y_train, y_test = train_test_split(x,y,test_size= 0.2 , random_state = 110)"
KC2,"def minkowsky(x,y,p): /   return np.power(sum(np.abs(x-y)**p), 1/p)  /// def distances(sample,x_train, y_train,p,k): /  distances = [] /  for i in range(len(x_train)): / ind     distance = minkowsky(sample,x_train[i],p) / ind     distances.append(distance) / ri   idx = np.argsort(distances)/   pred = np.argmax(np.bincount(y_train[idx][:k])) /  return pred "
KC3 ,"def knn_class(x_test, x_train, y_test,y_train,p,k): /   preds = [] /   for s in range(len(x_test)): / ind     pred = distances(x_test[s],x_train,y_train, p,k) / ind     preds.append(pred)/ ri   return accuracy_score(y_test,preds) / tested knn_class(x_test, x_train, y_test,y_train,2,3)"
errors class,"errors = []  for k in range(1,20): /   error = knn_class(x_test, x_train, y_test,y_train,2,k=k) /  errors.append(error)  // import matplotlib.pyplot as plt /plt.plot(range(1,20),errors/plt.show"
error regre,"errors = []  /for k in range(1,20): /  error = knn_regression(x_test, x_train, y_test,y_train,p,k=k) /  errors.append(error) /// import matplotlib.pyplot as plt /plt.plot(range(1,20),errors) /plt.show"
DISTANCES,"def manhatan(x,y):   return sum(np.abs(x-y)) / def euclidean(x,y):   return np.sqrt(sum((x-y)**2)) / def minkowsky(x,y,p):   return np.power(sum(np.abs(x-y)**p), 1/p)"
best comb p-k ,"#BEST COMBINATION OF K AND P  / errors = []  / for p in range(1,6): /  for k in range(1,20): /    error = knn_regression(x_test, x_train, y_test,y_train,p=p,k=k) /    errors.append(error)"
best error ,"best_error = 1 /errors = []  /for p in range(1,6): /  for k in range(1,20):    /    knn_reg = KNeighborsRegressor(n_neighbors=k, p=p)/    knn_reg.fit(x_train,y_train)/     pred = knn_reg.predict(x_test) /    error = (mean_squared_error(y_test,pred)) /     if error < best_error: /      best_error=error /      best_k = k /      best_p = p  /// prueba print(best_error) print(best_k) print(best_p)"
r2,"def r_sqr(y, pred):   test_r = 1 - (np.sum((y-pred)*2))/(np.sum((y-np.mean(y))*2))   return test_r"
dt with loop and depth ,"fun =np.random.default_rng(23) / x_dt= fun.uniform(2,20,10) /y_dt= fun.uniform(2,20,10)/// x_dt = np.array(x_dt).reshape(-1, 1) /// from sklearn.tree import DecisionTreeRegressor / from sklearn.metrics import mean_absolute_error //   errors = []     /  for i in range(10):/     X_train, X_test, y_train, y_test = \         train_test_split(x_dt, y_dt, test_size=0.20) /     dt = DecisionTreeRegressor(max_depth=2) /     dt.fit(X_train, y_train) /     y_predicted = dt.predict(X_test) /    e = mean_absolute_error(y_test,y_predicted) /    print(f"" ${e:.0f}"", end='')     errors.append(e) / ri print()  / noisy_avg_mae = np.mean(errors) /print(f""Average validation MAE ${noisy_avg_mae:.0f}"")"
